# AntNLP Seminar for PLMs -- 2022 Fall

Time: **06:00 pm - 8:00 pm, Thursday**

Venue: B914, Science Building.

Welcome to AntNLP Seminar for PLMs 2022 Fall. : )

## Presenters
- Our seminar scheduler is mostly adapted from following courses, feel free to find the course materials from them:
    - JHU CSCI 601.771: [Self-supervised Statistical Models](http://self-supervised.cs.jhu.edu/)
    - COS 597G (Fall 2022): [Understanding Large Language Models](https://www.cs.princeton.edu/courses/archive/fall22/cos597G/)
    - Stanford CS324 : [Large Language Models](https://stanford-cs324.github.io/winter2022/lectures/)
- Each talk is provided a given topic and some key phrases. Your talk should focus on the topic and introduce some related works. We list some reference papers to read. Try to include more high-impact papers in your talk, and you can add or remove some papers if you want. One important point is that you need to sort out the similarities and differences between the methods. Good Luck !

## Non-Presenters
- Please read the abstract/introduction sections before the seminar.

## Agenda
Week | Date  | Speaker | Topic     |Paper      | Key Phrase |Slides
---- | ----  | ----	   | ---- 	   | ----      | ----      | ---- 
1 	 | 10.20  | 李雨倩   | Preliminaries: Past, Architectures, Pre-training, Capabilities | 1. [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (Transformer) <br>   2.  [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf) (Bert) <br> 3. [ Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) (OpenAI GPT) <br> 4. [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692.pdf) (RoBERTa) <br> 5. [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/pdf/2003.10555.pdf) (ELECTRA)|   transformer; encoder-only models|[Slides](https://github.com/AntNLP/seminar/tree/master/2022Fall_PLM/week1/ppt.pptx)
2 	 | 10.27  | 刘燕婷	| Other Pretraining Language Models I|1. [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf) (T5)<br> 2. [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461.pdf) (BART) <br> 3. [ mT5: A massively multilingual pre-trained text-to-text transformer](https://arxiv.org/pdf/2010.11934.pdf) (mT5) <br> 4. [AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2seq Model](https://arxiv.org/pdf/2208.01448.pdf) (AlexaTM) | encoder-decoder models |[Slides](https://github.com/AntNLP/seminar/tree/master/2022Fall_PLM/week2/ppt.pptx)
3 	 | 11.10  |  杜威    | Other Pretraining Language Models II          | 1. [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)(GPT3) <br> 2. [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (GPT2) <br> 3. [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/pdf/2204.02311.pdf) (PaLM) <br> 4. [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/pdf/2205.01068.pdf) (OPT)        |   decoder-only models        |  [Slides](https://github.com/AntNLP/seminar/tree/master/2022Fall_PLM/week3/Decoder_only_PLM.pdf)
4 	 | 11.17 | 丁炫文   |  Prompting for few-shot learning |1. [ Making Pre-trained Language Models Better Few-shot Learners](https://arxiv.org/pdf/2012.15723.pdf) <br> 2. [ How Many Data Points is a Prompt Worth?](https://arxiv.org/pdf/2103.08493.pdf) <br> 3. [Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference](https://arxiv.org/pdf/2001.07676.pdf) <br> 4. [True Few-Shot Learning with Language Models](https://arxiv.org/pdf/2105.11447.pdf) <br> 5. [Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models](https://arxiv.org/pdf/2106.13353.pdf) |  PET         |  [Slides](https://github.com/AntNLP/seminar/tree/master/2022Fall_PLM/week4/slides.pdf)  
5   | 11.24/12.1 |  汪杰    | Prompting as parameter-efficient fine-tuning | 1.[Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/pdf/2101.00190.pdf)<br> 2. [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf) <br> 3.[ Factual Probing Is [MASK]: Learning vs. Learning to Recall](https://arxiv.org/pdf/2104.05240.pdf) <br> 4.[P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)<br> 5. [Towards a Unified View of Parameter-Efficient Transfer Learning](https://arxiv.org/pdf/2110.04366.pdf) | parameter efficient | [Slides](https://github.com/AntNLP/seminar/tree/master/2022Fall_PLM/week5/slides.pdf)
6 	 | 12.8 | 李雨倩 | In-context learning and limits I        | 1.  [Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity](https://arxiv.org/abs/2104.08786) <br> 2. [On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model](https://arxiv.org/abs/2204.13509) <br> 3. [Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://arxiv.org/pdf/2202.12837.pdf) |  In-context learning         | [Slides](https://github.com/AntNLP/seminar/tree/master/2022Fall_PLM/week6/slides.pdf)
7   | 12.15 | 刘燕婷 | In-context learning and limits II   |  1. [Impact of Pretraining Term Frequencies on Few-Shot Reasoning](https://arxiv.org/pdf/2202.07206.pdf) <br> 2. [Do Prompt-Based Models Really Understand the Meaning of their Prompts?](https://arxiv.org/abs/2109.01247)<br> 3.[What Makes Good In-Context Examples for GPT-3?](https://arxiv.org/pdf/2101.06804.pdf) |  In-context learning         |
8 | 12.22 |  丁炫文  | Data Memorization in Models I  | 1.[Extracting Training Data from Large Language Models](https://arxiv.org/pdf/2012.07805.pdf) <br> 2. [Quantifying Memorization Across Neural Language Models](https://arxiv.org/abs/2202.07646) <br> 3.[Counterfactual Memorization in Neural Language Models](https://arxiv.org/abs/2112.12938) | memorization
9 | 12.29 |  刘宇芳  | Data Memorization in Models II |  1. [Data Contamination: From Memorization to Exploitation](https://arxiv.org/abs/2203.08242) <br> 2. [Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models](https://arxiv.org/abs/2205.10770) <br> 3. [The Privacy Onion Effect: Memorization is Relative](https://openreview.net/pdf?id=ErUlLrGaVEU) | memorization
10 | 1.5 |  杜威   | Retrieval-based LMs I | 1.[REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909)<br> 2.[Improving language models by retrieving from trillions of tokens](https://arxiv.org/pdf/2112.04426.pdf)<br> 3. [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf) | retrieval from memory
11 | 1.12 | 杨晰    | Retrieval-based LMs II | 1.[Generalization through Memorization: Nearest Neighbor Language Models](https://arxiv.org/pdf/1911.00172.pdf)<br> 2.[Training Language Models with Memory Augmentation](https://arxiv.org/pdf/2205.12674.pdf)<br> 3.[Few-shot Learning with Retrieval Augmented Language Models](https://arxiv.org/abs/2208.03299) | retrieval from memory
<!--
8   | 12.15 |        | Calibration of prompting LLMs I | 1. [Calibrate Before Use: Improving Few-Shot Performance of Language Models](https://arxiv.org/pdf/2102.09690.pdf) <br> 2. [Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right](https://arxiv.org/pdf/2104.08315.pdf) <br> 3.[An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels](https://arxiv.org/abs/2203.11364) | contextual calibration for prompts | 
9   | 12.22 |        | Calibration of prompting LLMs II | 1. [How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering](https://arxiv.org/pdf/2012.00955.pdf) <br> 2. [Language Models (Mostly) Know What They Know](https://arxiv.org/pdf/2207.05221.pdf) | contextual calibration for prompts | 
12   | 1.19  |     | Chain of Thought Reasoning   | 1.[Chain of Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/pdf/2201.11903.pdf) <br> 2. [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/pdf/2205.11916.pdf) <br> 3. [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/pdf/2203.11171.pdf) 
13  | 1.19  |    | Privacy Risks in Models | 1.[Deduplicating Training Data Mitigates Privacy Risks in Language Models](https://arxiv.org/abs/2202.06539) <br> 2.[Large Language Models Can Be Strong Differentially Private Learners](https://arxiv.org/pdf/2110.05679.pdf) <br> 3.[Recovering Private Text in Federated Learning of Language Models](https://arxiv.org/pdf/2205.08514.pdf) <br> 4. [Differentially Private Fine-tuning of Language Models](https://arxiv.org/abs/2110.06500) <br> 5.[What Does it Mean for a Language Model to Preserve Privacy?](https://arxiv.org/pdf/2202.05520.pdf) | privacy protection
14 | 1.26  |    | Social Harms: Bias & Toxicity | 1. [ Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models](https://papers.nips.cc/paper/2021/file/1531beb762df4029513ebf9295e0d34f-Paper.pdf) <br> 2.[RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models](https://arxiv.org/pdf/2009.11462.pdf)<br> 3. [Challenges in Detoxifying Language Models](https://arxiv.org/pdf/2109.07445.pdf) <br>4.[Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP](https://arxiv.org/pdf/2103.00453.pdf) <br> 5.[TruthfulQA: Measuring How Models Mimic Human Falsehoods](https://arxiv.org/abs/2109.07958) | bias; toxicity
-->

---
## F.A.Q.

1. How to fill the slots and upload your slides?
- [creating-a-pull-request-from-a-fork/](https://help.github.com/articles/creating-a-pull-request-from-a-fork/)
- or you can contact:
  - Yufang Liu <yfliu.antnlp@gmail.com>
  - Wei Du <52265901025@ecnu.stu.edu.cn>
- any quesitons, please feel free to contact us.