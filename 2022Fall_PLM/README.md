# AntNLP Seminar for PLMs -- 2022 Fall

Time: **06:00 pm - 8:00 pm, Thursday**

Venue: B914, Science Building.

Welcome to AntNLP Seminar for PLMs 2022 Fall. : )

## Presenters
- Our seminar scheduler is mostly adapted from following courses, feel free to find the course materials from them:
    - JHU CSCI 601.771: [Self-supervised Statistical Models](http://self-supervised.cs.jhu.edu/)
    - COS 597G (Fall 2022): [Understanding Large Language Models](https://www.cs.princeton.edu/courses/archive/fall22/cos597G/)
    - Stanford CS324 : [Large Language Models](https://stanford-cs324.github.io/winter2022/lectures/)
- Each talk is provided a given topic and some key phrases. Your talk should focus on the topic and introduce some related works. We list some reference papers to read. Try to include more high-impact papers in your talk, and you can add or remove some papers if you want. One important point is that you need to sort out the similarities and differences between the methods. Good Luck !

## Non-Presenters
- Please read the abstract/introduction sections before the seminar.

## Agenda
Week | Date  | Speaker | Topic     |Paper      | Key Phrase |Slides
---- | ----  | ----	   | ---- 	   | ----      | ----      | ---- 
1 	 | 10.20  | 李雨倩   | Preliminaries: Past, Architectures, Pre-training, Capabilities | 1. [Attention Is All You Need](https://arxiv.org/abs/1706.03762) (Transformer) <br>   2.  [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf) (Bert) <br> 3. [ Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) (OpenAI GPT) <br> 4. [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692.pdf) (RoBERTa) <br> 5. [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/pdf/2003.10555.pdf) (ELECTRA)|   transformer; encoder-only models|[Slides](https://github.com/AntNLP/seminar/tree/master/2022Fall_PLM/week1/ppt.pptx)
2 	 | 10.27  | 刘燕婷	| Other Pretraining Language Models I|1. [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683.pdf) (T5)<br> 2. [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/pdf/1910.13461.pdf) (BART) <br> 3. [ mT5: A massively multilingual pre-trained text-to-text transformer](https://arxiv.org/pdf/2010.11934.pdf) (mT5) <br> 4. [AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2seq Model](https://arxiv.org/pdf/2208.01448.pdf) (AlexaTM) | encoder-decoder models |
3 	 | 11.3  |  杜威    | Other Pretraining Language Models II          | 1. [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)(GPT3) <br> 2. [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) (GPT2) <br> 3. [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/pdf/2204.02311.pdf) (PaLM) <br> 4. [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/pdf/2205.01068.pdf) (OPT)        |   decoder-only models        |
4 	 | 11.10 | 丁炫文   |  Prompting for few-shot learning |1. [ Making Pre-trained Language Models Better Few-shot Learners](https://arxiv.org/pdf/2012.15723.pdf) <br> 2. [ How Many Data Points is a Prompt Worth?](https://arxiv.org/pdf/2103.08493.pdf) <br> 3. [Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference](https://arxiv.org/pdf/2001.07676.pdf) <br> 4. [True Few-Shot Learning with Language Models](https://arxiv.org/pdf/2105.11447.pdf) <br> 5. [Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models](https://arxiv.org/pdf/2106.13353.pdf) |  PET         |    
5 	 | 11.17 | 汪杰 | In-context learning and limits         | 1.  [Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity](https://arxiv.org/abs/2104.08786) <br> 2. [On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model](https://arxiv.org/abs/2204.13509) <br> 3. [Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://arxiv.org/pdf/2202.12837.pdf)<br> 4. [Impact of Pretraining Term Frequencies on Few-Shot Reasoning](https://arxiv.org/pdf/2202.07206.pdf) <br> 5. [Do Prompt-Based Models Really Understand the Meaning of their Prompts?](https://arxiv.org/abs/2109.01247) |  In-context learning         |

---
## F.A.Q.

1. How to fill the slots and upload your slides?
- [creating-a-pull-request-from-a-fork/](https://help.github.com/articles/creating-a-pull-request-from-a-fork/)
- or you can contact:
  - Yufang Liu <yfliu.antnlp@gmail.com>
  - Wei Du <52265901025@ecnu.stu.edu.cn>
- any quesitons, please feel free to contact us.