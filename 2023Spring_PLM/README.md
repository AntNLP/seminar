# AntNLP Seminar for PLMs -- 2023 Spring

Time: **09:00 am - 11:00 am, Friday**

Venue: B513, Science Building.

Welcome to AntNLP Seminar for PLMs 2023 Spring. : )

## Presenters
- Our seminar scheduler is mostly adapted from following courses, feel free to find the course materials from them:
    - JHU CSCI 601.771: [Self-supervised Statistical Models](http://self-supervised.cs.jhu.edu/)
    - COS 597G (Fall 2022): [Understanding Large Language Models](https://www.cs.princeton.edu/courses/archive/fall22/cos597G/)
    - Stanford CS324 : [Large Language Models](https://stanford-cs324.github.io/winter2022/lectures/)
- Each talk is provided a given topic and some key phrases. Your talk should focus on the topic and introduce some related works. Good Luck !

## Non-Presenters
- Please read the abstract/introduction sections before the seminar.

## Agenda
Week | Date  | Speaker | Topic     |Paper      |Slides
---- | ----  | ----	   | ---- 	   | ----      | ---- 
1 | 3.3 |  杜威   | Retrieval-based LMs  | 1.[REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909)<br> 2.[Improving language models by retrieving from trillions of tokens](https://arxiv.org/pdf/2112.04426.pdf)<br> 3. [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf) | [Slides](https://github.com/AntNLP/seminar/tree/master/2023Spring_PLM/week1/20230302.pptx)
2 | 3.10 | 刘宇芳    | Retrieval-based LMs | 1.[Generalization through Memorization: Nearest Neighbor Language Models](https://arxiv.org/pdf/1911.00172.pdf)<br> 2.[Training Language Models with Memory Augmentation](https://arxiv.org/pdf/2205.12674.pdf)<br> 3.[Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval](https://proceedings.mlr.press/v162/alon22a/alon22a.pdf) | [Slides](https://github.com/AntNLP/seminar/tree/master/2023Spring_PLM/week2/0310.pdf)
3 | 3.17 | 刘燕婷   |   Chain of Thought  |1.[Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://openreview.net/pdf?id=_VjQlMeSB_J) <br> 2.[Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://openreview.net/pdf?id=1PL1NIMMrw)<br>3. [Iteratively Prompt Pre-trained Language Models for Chain of Thought](https://aclanthology.org/2022.emnlp-main.174.pdf)|[Slides](https://github.com/AntNLP/seminar/tree/master/2023Spring_PLM/week3/cot.pptx)
4 | 3.31 | 王志承  | Instruction Tuning | [Crosslingual Generalization through Multitask Finetuning](https://arxiv.org/pdf/2211.01786.pdf) | [Slides](https://github.com/AntNLP/seminar/tree/master/2023Spring_PLM/week4/BLOOMZ.pdf) 
5 | 4.14 | 李雨倩 | Bias and Toxicity | 1.[On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922) <br> 2.[REALTOXICITYPROMPTS:Evaluating Neural Toxic Degeneration in Language Models](https://aclanthology.org/2020.findings-emnlp.301.pdf) <br> 3.[Whose Language Counts as High Quality?Measuring Language Ideologies in Text Data Selection](https://arxiv.org/pdf/2201.10474.pdf) | [Slides](https://github.com/AntNLP/seminar/tree/master/2023Spring_PLM/week5/bias&toxicity.pptx)
6 | 4.21 | 汪杰 | Transformer Structures | 1.[Longformer: The Long-Document Transformer](https://arxiv.org/pdf/2004.05150) | [Slides](https://github.com/AntNLP/seminar/tree/master/2023Spring_PLM/week6/wj.pdf)