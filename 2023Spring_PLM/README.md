# AntNLP Seminar for PLMs -- 2023 Spring

Time: **09:00 am - 11:00 am, Friday**

Venue: B513, Science Building.

Welcome to AntNLP Seminar for PLMs 2023 Spring. : )

## Presenters
- Our seminar scheduler is mostly adapted from following courses, feel free to find the course materials from them:
    - JHU CSCI 601.771: [Self-supervised Statistical Models](http://self-supervised.cs.jhu.edu/)
    - COS 597G (Fall 2022): [Understanding Large Language Models](https://www.cs.princeton.edu/courses/archive/fall22/cos597G/)
    - Stanford CS324 : [Large Language Models](https://stanford-cs324.github.io/winter2022/lectures/)
- Each talk is provided a given topic and some key phrases. Your talk should focus on the topic and introduce some related works. Good Luck !

## Non-Presenters
- Please read the abstract/introduction sections before the seminar.

## Agenda
Week | Date  | Speaker | Topic     |Paper      |Slides
---- | ----  | ----	   | ---- 	   | ----      | ---- 
1 | 3.3 |  杜威   | Retrieval-based LMs  | 1. [REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/2002.08909)<br> 2. [Improving language models by retrieving from trillions of tokens](https://arxiv.org/pdf/2112.04426.pdf)<br> 3. [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf) | [Slides](https://github.com/AntNLP/seminar/tree/master/2023Spring_PLM/week1/20230302.pptx)
2 | 3.10 | 刘宇芳    | Retrieval-based LMs | 1. [Generalization through Memorization: Nearest Neighbor Language Models](https://arxiv.org/pdf/1911.00172.pdf)<br> 2. [Training Language Models with Memory Augmentation](https://arxiv.org/pdf/2205.12674.pdf)<br> 3. [Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval](https://proceedings.mlr.press/v162/alon22a/alon22a.pdf) | [Slides](https://github.com/AntNLP/seminar/tree/master/2023Spring_PLM/week2/0310.pdf)
3 | 3.17 | 刘燕婷   |   Chain of Thought  |1. [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://openreview.net/pdf?id=_VjQlMeSB_J) <br> 2. [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://openreview.net/pdf?id=1PL1NIMMrw)<br>3. [Iteratively Prompt Pre-trained Language Models for Chain of Thought](https://aclanthology.org/2022.emnlp-main.174.pdf)|[Slides](https://github.com/AntNLP/seminar/tree/master/2023Spring_PLM/week3/cot.pptx)
4 | 3.31 | 王志承  | Instruction Tuning | [Crosslingual Generalization through Multitask Finetuning](https://arxiv.org/pdf/2211.01786.pdf) | [Slides](https://github.com/AntNLP/seminar/tree/master/2023Spring_PLM/week4/BLOOMZ.pdf) 
5 | 4.14 | 李雨倩 | Bias and Toxicity | 1. [On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922) <br> 2.    [REALTOXICITYPROMPTS:Evaluating Neural Toxic Degeneration in Language Models](https://aclanthology.org/2020.findings-emnlp.301.pdf) <br> 3. [Whose Language Counts as High Quality?Measuring Language Ideologies in Text Data Selection](https://arxiv.org/pdf/2201.10474.pdf) | [Slides](https://github.com/AntNLP/seminar/tree/master/2023Spring_PLM/week5/bias&toxicity.pptx)
6 | 4.21 | 汪杰 | Transformer Structures | 1. [Longformer: The Long-Document Transformer](https://arxiv.org/pdf/2004.05150) | [Slides](https://github.com/AntNLP/seminar/tree/master/2023Spring_PLM/week6/wj.pdf)
7 | 5.12 | 纪焘 | Dependency Syntactic Parsing |-|-
8 | 6.2 | 朱威 | Information Extraction| 1. [Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness](https://arxiv.org/pdf/2304.11633.pdf) <br> 2. [CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors](https://arxiv.org/pdf/2305.05711.pdf) | [Slides](https://github.com/AntNLP/seminar/tree/master/2023Spring_PLM/week8/week8.pptx)
9 | 6.9 | 郑焕然 | Non-Autoregressive Generation | 1. [Non-Autoregressive Neural Machine Translation](https://openreview.net/forum?id=B1l8BtlCb) <br> 2. [Mask-Predict: Parallel Decoding of Conditional Masked Language Models](https://aclanthology.org/D19-1633.pdf) <br> 3. [Glancing Transformer for Non-Autoregressive Neural Machine Translation](https://doi.org/10.18653/v1/2021.acl-long.155) <br> 4. [Lossless Acceleration for Seq2seq Generation with Aggressive Decoding](https://doi.org/10.48550/arXiv.2205.10350) | [Slides](https://github.com/AntNLP/seminar/tree/master/2023Spring_PLM/week9/week9.pptx)
10 | 6.16 | 王鹏飞 | Vision-Language Pre-training | 1. [MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models](https://arxiv.org/pdf/2304.10592.pdf) <br> 2. [mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video](https://arxiv.org/pdf/2302.00402.pdf) | [Slides](https://github.com/AntNLP/seminar/tree/master/2023Spring_PLM/week10/20230616-wpf.pptx)
11 | 6.30 | 丁炫文 | Knowledge Inheritance | 1. [Knowledge Inheritance for Pre-trained Language Models](https://aclanthology.org/2022.naacl-main.288.pdf) <br> 2. [bert2BERT: Towards Reusable Pretrained Language Models](https://aclanthology.org/2022.acl-long.151.pdf) <br> 3. [ELLE: Efficient Lifelong Pre-training for Emerging Data](https://aclanthology.org/2022.findings-acl.220.pdf) | [Slides](https://github.com/AntNLP/seminar/tree/master/2023Spring_PLM/week11/Knowledge_Inheritance.pdf)
12 | 7.7 | 汪杰 | Long Context Language Models | 1. [Extending Context Window of Large Language Models via Positional Interpolation](https://arxiv.org/pdf/2306.15595.pdf) <br> 2. [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/pdf/2108.12409.pdf) | [Slides](https://github.com/AntNLP/seminar/tree/master/2023Spring_PLM/week12/slides.pdf)
13 | 7.28| 刘宇芳|Sparse Mixture of Experts for Modularity | 1. [ModuleFormer: Learning Modular Large Language Models From Uncurated Data](https://arxiv.org/pdf/2306.04640) <br> 2. [Sparse MoE as the New Dropout: Scaling Dense and Self-Slimmable Transformers](https://openreview.net/pdf?id=w1hwFUb_81) | [Slides](https://github.com/AntNLP/seminar/tree/master/2023Spring_PLM/week13/0728.pdf)
14 | 8.10 | 杜威 | Retrieval-Based LM | 1. [Long-range Language Modeling with Self-retrieval](https://arxiv.org/pdf/2306.13421)<br> 2.[Copy is All You Need](https://openreview.net/pdf?id=CROlOA9Nd8C) | [Slides](https://github.com/AntNLP/seminar/tree/master/2023Spring_PLM/week14/slides.pdf)
15 | 8.17 | 刘燕婷 | Retrieval-Based LM  |[Unlimiformer: Long-Range Transformers withUnlimited Length Input](https://arxiv.org/pdf/2305.01625.pdf) | [Slides](https://github.com/AntNLP/seminar/tree/master/2023Spring_PLM/week15/slides.pptx)
