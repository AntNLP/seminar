# Statistical Natural Language Processing Seminar -- 2023 Spring

Time: **9:50, Thursday**

Venue: Tin Ka Ping Building 234 (田家炳楼234)

Welcome to StatNLP Seminar 2023 Spring. : )

## On Papers

- Please choose recent papers (2023, 2022, 2021) from top NLP/AI venues. A (incomplete) list is
  - NLP: ACL, TACL, EMNLP, NAACL, EACL
  - ML: ICML, NeurIPS, AISTATS, JMLR, ICLR
  - AI: AAAI, IJCAI
  - IR/DM: SIGIR, CIKM, WSDM, KDD, WWW
- While we are interested in a broad range of NLP/AI topics, this year we will pay special attentions on two themes
  - foundation models (pre-training models)
  - explaining methods
- We hope that, with high probability, you choose papers from [here](https://github.com/AntNLP/seminar/blob/master/2023Spring_StatNLP/2023-paper-list.md)
- other materials with broad interests are welcome (e.g., tutorials form top conferences, high-quality surveys).

## For Presenters

- Please fill your slots in the Agenda at least one week before your presentation.

  - Please format Paper fields with *[venue+year]title* (e.g. [ACL21]A Good Paper).
- Please upload your slides, and add links to them in Slides fields.
  
- Besides technical novelties, please give enough background knowledge in case people are unfamiliar with your topic.

- It would be great to keep your presentation within 60 min.

## For Audiences

- Please read abstract/introduction sections before the seminar.

## Agenda



| Week | Date | Speaker | Paper | Materials |
| ---- | ---- | ------- | ----- | --------- |
| 1    | 3.2  |    停电     |       |           |
| 2    | 3.9  |    李雨倩<br>汪杰     |   [\[EMNLP22\]Autoregressive Structured Prediction with Language Models](https://aclanthology.org/2022.findings-emnlp.70/) <br> [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)  |    [Slide](https://github.com/AntNLP/seminar/edit/master/2023Spring_StatNLP/week2/)       |
| 3    | 3.16  |    刘燕婷<br>兰孟烨   |   [\[ACL22\]Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction](https://aclanthology.org/2022.acl-long.466/)<br>[\[EMNLP22\]Continual Training of Language Models for Few-Shot Learning](https://aclanthology.org/2022.emnlp-main.695/)    |    [Slide](https://github.com/AntNLP/seminar/edit/master/2023Spring_StatNLP/week3/)       |
| 4    | 3.23  |    顾轶洋<br>李雍   |   [\[ACL22\]There Are a Thousand Hamlets in a Thousand People’s Eyes: Enhancing Knowledge-grounded Dialogue with Personal Memory](https://aclanthology.org/2022.acl-long.270/)<br>[\[EMNLP22\]InstructDial: Improving Zero and Few-shot Generalization in Dialogue through Instruction Tuning](https://aclanthology.org/2022.emnlp-main.33/)    |    [Slide](https://github.com/AntNLP/seminar/edit/master/2023Spring_StatNLP/week4/)       |
| 5    | 3.30  |    殷炜<br>魏旨航   |   [\[ACL22\]Overcoming Catastrophic Forgetting beyond Continual Learning: Balanced Training for Neural Machine Translation](https://aclanthology.org/2022.acl-long.143/)<br>[\[ACL22\]FlipDA: Effective and Robust Data Augmentation for Few-Shot Learning](https://aclanthology.org/2022.acl-long.592/)    |    [Slide](https://github.com/AntNLP/seminar/edit/master/2023Spring_StatNLP/week5/)       |
| 6    | 4.6  |    陈可迪<br>张燮弛   |   [GPTEVAL: NLG Evaluation using GPT-4 with Better Human Alignment](https://arxiv.org/abs/2303.16634)<br>[SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models](https://arxiv.org/abs/2303.08896)    |    [Slide](https://github.com/AntNLP/seminar/edit/master/2023Spring_StatNLP/week6/)       |
| 7    | 4.13  |    李龙<br>邹佳玲<br>丁炫文   |   [\[ACL22\]Faithful or Extractive? On Mitigating the Faithfulness-Abstractiveness Trade-off in Abstractive Summarization](https://aclanthology.org/2022.acl-long.100/)<br>[\[ACL22\]PPT: Pre-trained Prompt Tuning for Few-shot Learning](https://aclanthology.org/2022.acl-long.576/) <br>[\[EMNLP22\]Bi-Directional Iterative Prompt-Tuning for Event Argument Extraction](https://aclanthology.org/2022.emnlp-main.419/)    |    [Slide](https://github.com/AntNLP/seminar/edit/master/2023Spring_StatNLP/week7/)       |
| 8    | 4.20  |    张启<br>白耿龙<br>楼城灿   |   [\[ACL22\]Mix and Match: Learning-free Controllable Text Generationusing Energy Language Models](https://aclanthology.org/2022.acl-long.31/)<br>[\[ACL22\]Can Prompt Probe Pretrained Language Models? Understanding the Invisible Risks from a Causal View](https://aclanthology.org/2022.acl-long.398/) <br>[\[ICLR23\]Copy is All You Need](https://openreview.net/pdf?id=CROlOA9Nd8C)    |    [Slide](https://github.com/AntNLP/seminar/edit/master/2023Spring_StatNLP/week8/)       |
| 9    | 4.27  |    孟子洋<br>戴煜   |   [\[ACL22\]Kronecker Decomposition for GPT Compression](https://aclanthology.org/2022.acl-short.24.pdf)<br>[\[SIGIR22\]Knowledge Graph Contrastive Learning for Recommendation](https://arxiv.org/abs/2205.00976)    |    [Slide](https://github.com/AntNLP/seminar/edit/master/2023Spring_StatNLP/week9/)       |
| 10    | 5.4  |    陈洋<br>Sabina<br>董文洁   |   [\[ACL22\]Generated Knowledge Prompting for Commonsense Reasoning](https://aclanthology.org/2022.acl-long.225.pdf)<br>[\[ACL22\]Richer Countries and Richer Representations](https://aclanthology.org/2022.findings-acl.164.pdf)<br>[\[EMNLP22\]Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://aclanthology.org/2022.emnlp-main.759/)    |    [Slide](https://github.com/AntNLP/seminar/edit/master/2023Spring_StatNLP/week10/)       |
| 11    | 5.11  |    魏旨航<br>刘燕婷   |   [\[ACL22\]Noisy Channel Language Model Prompting for Few-Shot Text Classification](https://aclanthology.org/2022.acl-long.365.pdf)<br>[\[ACL22\]Dynamic Prefix-Tuning for Generative Template-based Event Extraction](https://aclanthology.org/2022.acl-long.358.pdf)    |    [Slide](https://github.com/AntNLP/seminar/edit/master/2023Spring_StatNLP/week11/)       |
| 12    | 5.18  |    兰孟烨<br>陈可迪<br>张燮弛  |   [\[EMNLP22\]Natural Language to Code Translation with Execution](https://aclanthology.org/2022.emnlp-main.231.pdf)<br>[\[EMNLP22\]Calibrating Factual Knowledge in Pretrained Language Models](https://aclanthology.org/2022.findings-emnlp.438/)<br>[\[ACL22\]GLM: General Language Model Pretraining with Autoregressive Blank Infilling](https://aclanthology.org/2022.acl-long.26/)    |    [Slide](https://github.com/AntNLP/seminar/edit/master/2023Spring_StatNLP/week12/)       |
| 13    | 5.25  |    殷炜<br>邹佳玲<br>白耿龙  |   [\[ACL22\]Towards Making the Most of Cross-Lingual Transfer for Zero-Shot Neural Machine Translation](https://aclanthology.org/2022.acl-long.12/)<br>[\[ACL22\]bert2BERT: Towards Reusable Pretrained Language Models](https://aclanthology.org/2022.acl-long.151/)<br>[\[EMNLP22\]Are All Spurious Features in Natural Language Alike? An Analysis through a Causal Lens](https://aclanthology.org/2022.emnlp-main.666.pdf)    |    [Slide](https://github.com/AntNLP/seminar/edit/master/2023Spring_StatNLP/week13/)       |
| 14    | 6.1  |   Sabina<br>顾轶洋<br>李雍  |   [\[ICLR23\]Ask Me Anything: A simple strategy for prompting language models](https://arxiv.org/abs/2210.02441)<br>[\[ICLR23\]Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language Models](https://arxiv.org/abs/2210.16433)<br>[\[ACL22\]Meta-learning via Language Model In-context Tuning](https://aclanthology.org/2022.acl-long.53/)    |    [Slide](https://github.com/AntNLP/seminar/edit/master/2023Spring_StatNLP/week14/)       |
| 15    | 6.8  |   戴煜<br>楼城灿<br>孟子洋  |   [\[ACL22\]CONTAINER: Few-Shot Named Entity Recognition via Contrastive Learning](https://aclanthology.org/2022.acl-long.439.pdf)<br>[\[ICLR23\]Compositional Task Representations for Large Language Models](https://arxiv.org/abs/2209.15003)<br>[\[EMNLP22\]How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers](https://aclanthology.org/2022.findings-emnlp.101/)    |    [Slide](https://github.com/AntNLP/seminar/edit/master/2023Spring_StatNLP/week15/)       |
| 16    | 6.15  |   汪杰<br>李龙<br>张启  |   [\[NACCL22\]Simple Local Attentions Remain Competitive for Long-Context Tasks](https://aclanthology.org/2022.naacl-main.144/)<br>[\[EACL23\]SWING : Balancing Coverage and Faithfulness for Dialogue Summarization](https://aclanthology.org/2023.findings-eacl.37/)<br>[\[ICLR23\]DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models](https://arxiv.org/abs/2210.08933)    |    [Slide](https://github.com/AntNLP/seminar/edit/master/2023Spring_StatNLP/week16/)       |
| 17    | 6.22  |    端午节     |       |           |
| 18    | 6.29  |   丁炫文<br>董文洁<br>李雨倩<br>陈洋  |   [\[ACL22\]Continual Few-shot Relation Learning via Embedding Space Regularization and Data Augmentation](https://aclanthology.org/2022.acl-long.198/)<br>[\[ICLR23\]Parameter-Efficient Fine-Tuning Design Spaces](https://arxiv.org/abs/2301.01821)<br>[\[ACL23\]DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models](https://arxiv.org/pdf/2211.15029.pdf)<br>[\[NAACL23\]AmbiPun: Generating Humorous Puns with Ambiguous Context](https://aclanthology.org/2022.naacl-main.77/)    |    [Slide](https://github.com/AntNLP/seminar/edit/master/2023Spring_StatNLP/week18/)       |
