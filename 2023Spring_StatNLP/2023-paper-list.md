## Few-shot/Zero-shot Learning, Self-supervised Learning, Contrastive Learning, Life-long Learning, Active Learning 
  - [ACL 22] Few-Shot Class-Incremental Learning for Named Entity Recognition 
  - [ACL 22] Meta-learning via Language Model In-context Tuning
  - [ACL 22] Overcoming Catastrophic Forgetting beyond Continual Learning: Balanced Training for Neural Machine Translation
  - [ACL 22] Cross-Lingual Contrastive Learning for Fine-Grained Entity Typing for Low-Resource Languages
  - [ACL 22] MELM: Data Augmentation with Masked Entity Language Modeling for Low-Resource NER
  - [ACL 22] Good Examples Make A Faster Learner: Simple Demonstration-based Learning for Low-resource NER
  - [ACL 22] Continual Few-shot Relation Learning via Embedding Space Regularization and Data Augmentation
  - [ACL 22] ConTinTin: Continual Learning from Task Instructions
  - [ACL 22] On Continual Model Refinement in Out-of-Distribution Data Streams
  - [ACL 22] Continual Sequence Generation with Adaptive Compositional Modules
  - [ACL 22] Noisy Channel Language Model Prompting for Few-Shot Text Classification
  - [ACL 22] Few-shot Named Entity Recognition with Self-describing Networks
  - [ACL 22] CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning
  - [ACL 22] PPT: Pre-trained Prompt Tuning for Few-shot Learning
  - [ACL 22] FlipDA: Effective and Robust Data Augmentation for Few-Shot Learning
  - [ACL 22] Inverse is Better! Fast and Accurate Prompt for Few-shot Slot Tagging
  - [ACL 22] A Simple yet Effective Relation Information Guided Approach for Few-Shot Relation Extraction
  - [ACL 22] ELLE: Efficient Lifelong Pre-training for Emerging Data
  - [ACL 22] Modular Domain Adaptation
  - [EMNLP 22] TemporalWiki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models
  - [EMNLP 22] Learning Robust Representations for Continual Relation Extraction via Adversarial Class Augmentation
  - [EMNLP 22] Continual Training of Language Models for Few-Shot Learning
  - [EMNLP 22] ProGen: Progressive Zero-shot Dataset Generation via In-context Feedback
  - [EMNLP 22] Semi-Supervised Lifelong Language Learning
  - [ICLR 22] Can BERT Refrain from Forgetting on Sequential Tasks? A Probing Study 

## Application
  - [ACL 22] QuoteR: A Benchmark of Quote Recommendation for Writing
  - [ACL 22] How can NLP Help Revitalize Endangered Languages? A Case Study and Roadmap for the Cherokee Language
  - [ACL 22] Leveraging Similar Users for Personalized Language Modeling with Limited Data
  - [ACL 22] GPT-D: Inducing Dementia-related Linguistic Anomalies by Deliberate Degradation of Artificial Neural Language Models
  - [ACL 22] Learning to Mediate Disparities Towards Pragmatic Communication
  - [ACL 22] Adaptive Testing and Debugging of NLP Models
  - [EMNLP 22] Gendered Mental Health Stigma in Masked Language Models

## Interpretability, Robustness, Compressing, Privacy
  - [ACL 22] Explanation Graph Generation via Pre-trained Language Models: An Empirical Study with Contrastive Learning 
  - [ACL 22] Structured Pruning Learns Compact and Accurate Models
  - [ACL 22] Probing as Quantifying Inductive Bias
  - [ACL 22] Probing Structured Pruning on Multilingual Pre-trained Models: Settings, Algorithms, and Efficiency 
  - [ACL 22] On the Sensitivity and Stability of Model Interpretations in NLP
  - [ACL 22] Contextual Representation Learning beyond Masked Language Modeling
  - [ACL 22] Probing for Predicate Argument Structures in Pretrained Language Models
  - [ACL 22] Unsupervised Dependency Graph Network
  - [ACL 22] Compression of Generative Pre-trained Language Models via Quantization
  - [ACL 22] A Comparative Study of Faithfulness Metrics for Model Interpretability Methods
  - [ACL 22] On the Calibration of Pre-trained Language Models using Mixup Guided by Area Under the Margin and Saliency
  - [ACL 22] Finding Structural Knowledge in Multimodal-BERT
  - [ACL 22] Can Prompt Probe Pretrained Language Models? Understanding the Invisible Risks from a Causal View
  - [ACL 22] Logic Traps in Evaluating Attribution Scores
  - [ACL 22] Can Explanations Be Useful for Calibrating Black Box Models?
  - [ACL 22] An Empirical Study of Memorization in NLP
  - [ACL 22] CLUES: A Benchmark for Learning Classifiers using Natural Language Explanations
  - [ACL 22] Is GPT-3 Text Indistinguishable from Human Text? Scarecrow: A Framework for Scrutinizing Machine Text
  - [ACL 22] Memorisation versus Generalisation in Pre-trained Language Models
  - [ACL 22] Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity
  - [ACL 22] How does the pre-training objective affect what large language models learn about linguistic properties?
  - [ACL 22] Kronecker Decomposition for GPT Compression
  - [ACL 22] Revisiting the Compositional Generalization Abilities of Neural Sequence Models
  - [ACL 22] Finding the Dominant Winning Ticket in Pre-Trained Language Models
  - [EMNLP 22] Interpreting Language Models with Contrastive Explanations
  - [EMNLP 22] Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection
  - [EMNLP 22] Calibration Meets Explanation: A Simple and Effective Approach for Model Confidence Estimates
  - [EMNLP 22] Language Contamination Helps Explains the Cross-lingual Capabilities of English Pretrained Models
  - [EMNLP 22] Analyzing the Mono- and Cross-Lingual Pretraining Dynamics of Multilingual Language Models
  - [EMNLP 22] Finding Dataset Shortcuts with Grammar Induction
  - [EMNLP 22] Just Fine-tune Twice: Selective Differential Privacy for Large Language Models
  - [EMNLP 22] Quantifying Privacy Risks of Masked Language Models Using Membership Inference Attacks
  - [EMNLP 22] Measuring the Mixing of Contextual Information in the Transformer
  - [EMNLP 22] Are representations built from the ground up? An empirical examination of local composition in language models
  - [EMNLP 22] Are All Spurious Features in Natural Language Alike? An Analysis through a Causal Lens
  - [EMNLP 22] Finding Skill Neurons in Pre-trained Transformer-based Language Models
  - [EMNLP 22] Can language models learn from explanations in context?
  - [EMNLP 22] Systematicity in GPT-3’s Interpretation of Novel English Noun Compounds
  - [EMNLP 22] How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers
  - [EMNLP 22] What Has Been Enhanced in my Knowledge-Enhanced Language Model?
  - [EMNLP 22] ER-Test: Evaluating Explanation Regularization Methods for Language Models
  - [EMNLP 22] Unsupervised Text Deidentification
  - [EMNLP 22] Probing for Constituency Structure in Neural Language Models
  - [ICLR 23] PINTO: Faithful Language Reasoning Using Prompted-Generated Rationales
  - [ICLR 23] Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 Small 

## Machine Translation, Multi-(Cross-)lingual Tasks
  - [ACL 22] Towards Making the Most of Cross-Lingual Transfer for Zero-Shot Neural Machine Translation
  - [ACL 22] DEEP: DEnoising Entity Pre-training for Neural Machine Translation
  - [ACL 22] Composable Sparse Fine-Tuning for Cross-Lingual Transfer
  - [ACL 22] Cross-Lingual Ability of Multilingual Masked Language Models: A Study of Language Structure
  - [ACL 22] Systematic Inequalities in Language Technology Performance across the World’s Languages
  - [ACL 22] Word Order Does Matter and Shuffled Language Models Know It
  - [ACL 22] Make the Best of Cross-lingual Transfer: Evidence from POS Tagging with over 100 Languages
  - [ACL 22] Local Languages, Third Spaces, and other High-Resource Scenarios
  - [EMNLP 22] Few-shot Learning with Multilingual Generative Language Models

## General Reasoning, Commonsense
  - [ACL 22] Things not Written in Text: Exploring Spatial Commonsense from Visual Signals
  - [ACL 22] Generated Knowledge Prompting for Commonsense Reasoning
  - [ACL 22] Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data
  - [ACL 22] Making Transformers Solve Compositional Tasks
  - [ACL 22] Can Transformer be Too Compositional? Analysing Idiom Processing in Neural Machine Translation
  - [EMNLP 22] Generating Natural Language Proofs with Verifier-Guided Search
  - [EMNLP 22] Reasoning Like Program Executors
  - [EMNLP 22] A Systematic Investigation of Commonsense Knowledge in Large Language Models
  - [EMNLP 22] Data-Efficient Concept Extraction from Pre-trained Language Models for Commonsense Explanation Generation

## Pre-trained Models, Embeddings
  - [ACL 22] AdapLeR: Speeding up Inference by Adaptive Length Reduction
  - [ACL 22] GLM: General Language Model Pretraining with Autoregressive Blank Infilling
  - [ACL 22] Prompt-Based Rule Discovery and Boosting for Interactive Weakly-Supervised Learning
  - [ACL 22] Expanding Pretrained Models to Thousands More Languages via Lexicon-based Adaptation
  - [ACL 22] Multi-Granularity Structural Knowledge Distillation for Language Model Compression
  - [ACL 22] bert2BERT: Towards Reusable Pretrained Language Models
  - [ACL 22] Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification
  - [ACL 22] Word2Box: Capturing Set-Theoretic Semantics of Words using Box Embeddings
  - [ACL 22] ClusterFormer: Neural Clustering Attention for Efficient and Effective Transformer
  - [ACL 22] Prompt-free and Efficient Few-shot Learning with Language Models
  - [ACL 22] Token Dropping for Efficient BERT Pretraining
  - [ACL 22] Fine- and Coarse-Granularity Hybrid Self-Attention for Efficient BERT
  - [ACL 22] SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer
  - [ACL 22] ∞-former: Infinite Memory Transformer
  - [ACL 22] Fully Hyperbolic Neural Networks
  - [ACL 22] SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing
  - [ACL 22] Pre-training and Fine-tuning Neural Topic Model: A Simple yet Effective Approach to Incorporating External Knowledge
  - [ACL 22] MSP: Multi-Stage Prompting for Making Pre-trained Language Models Better Translators
  - [ACL 22] XLM-E: Cross-lingual Language Model Pre-training via ELECTRA
  - [ACL 22] UniPELT: A Unified Framework for Parameter-Efficient Language Model Tuning
  - [ACL 22] Universal Conditional Masked Language Pre-training for Neural Machine Translation
  - [ACL 22] ABC: Attention with Bounded-memory Control
  - [ACL 22] LinkBERT: Pretraining Language Models with Document Links
  - [ACL 22] Few-Shot Learning with Siamese Networks and Label Tuning
  - [ACL 22] The Power of Prompt Tuning for Low-Resource Semantic Parsing
  - [ACL 22] An Embarrassingly Simple Method to Mitigate Undesirable Properties of Pretrained Language Model Tokenizers
  - [ACL 22] A Simple but Effective Pluggable Entity Lookup Table for Pre-trained Language Models
  - [ACL 22] NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better
  - [ACL 22] Reframing Instructional Prompts to GPTk’s Language
  - [ACL 22] How Pre-trained Language Models Capture Factual Knowledge? A Causal-Inspired Analysis
  - [ACL 22] Metadata Shaping: A Simple Approach for Knowledge-Enhanced Language Models
  - [ACL 22] Syntax-guided Contrastive Learning for Pre-trained Language Model
  - [ACL 22] Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models
  - [ACL 22] Factual Consistency of Multilingual Pretrained Language Models
  - [ACL 22] Controlling the Focus of Pretrained Language Generation Models
  - [EMNLP 22] Linearizing Transformer with Key-Value Memory
  - [EMNLP 22] Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference
  - [EMNLP 22] Language Model Decomposition: Quantifying the Dependency and Correlation of Language Models
  - [EMNLP 22] Revisiting Parameter-Efficient Tuning: Are We Really There Yet?
  - [EMNLP 22] Memory-assisted prompt editing to improve GPT-3 after deployment
  - [EMNLP 22] An Efficient Memory-Augmented Transformer for Knowledge-Intensive NLP Tasks
  - [EMNLP 22] Training Language Models with Memory Augmentation
  - [EMNLP 22] Invariant Language Modeling
  - [EMNLP 22] AdaMix: Mixture-of-Adaptations for Parameter-efficient Model Tuning
  - [EMNLP 22] InforMask: Unsupervised Informative Masking for Language Model Pretraining
  - [EMNLP 22] G-MAP: General Memory-Augmented Pre-trained Language Model for Domain Tasks
  - [EMNLP 22] ATTEMPT: Parameter-Efficient Multi-task Tuning via Attentional Mixtures of Soft Prompts
  - [EMNLP 22] Exploring Mode Connectivity for Pre-trained Language Models
  - [EMNLP 22] The Devil in Linear Transformer
  - [EMNLP 22] Parameter-Efficient Tuning Makes a Good Classification Head
  - [EMNLP 22] HashFormers: Towards Vocabulary-independent Pre-trained Transformers
  - [EMNLP 22] Few-shot Learning with Multilingual Generative Language Models
  - [EMNLP 22] Detecting Label Errors by Using Pre-Trained Language Models
  - [EMNLP 22] Automatic Document Selection for Efficient Encoder Pretraining
  - [EMNLP 22] Breakpoint Transformers for Modeling and Tracking Intermediate Beliefs
  - [EMNLP 22] Meta-Learning Fast Weight Language Models
  - [EMNLP 22] Fine-Tuning Pre-trained Transformers into Decaying Fast Weights
  - [EMNLP 22] Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?
  - [EMNLP 22] Pre-training Language Models with Deterministic Factual Knowledge
  - [EMNLP 22] Efficient Large Scale Language Modeling with Mixtures of Experts
  - [EMNLP 22] What Language Model to Train if You Have One Million GPU Hours?
  - [EMNLP 22] Contrastive Demonstration Tuning for Pre-trained Language Models
  - [EMNLP 22] A Few More Examples May Be Worth Billions of Parameters
  - [EMNLP 22] Late Prompt Tuning: A Late Prompt Could Be Better Than Many Prompts
  - [EMNLP 22] You can’t pick your neighbors, or can you? When and How to Rely on Retrieval in the kNN-LM
  - [EMNLP 22] Different Tunes Played with Equal Skill: Exploring a Unified Optimization Subspace for Parameter-Efficient Tuning
  - [EMNLP 22] On the Role of Bidirectionality in Language Model Pre-Training
  - [EMNLP 22] Improving Generalization of Pre-trained Language Models via Stochastic Weight Averaging
  - [EMNLP 22] Calibrating Factual Knowledge in Pretrained Language Models
  - [EMNLP 22] From Mimicking to Integrating: Knowledge Integration for Pre-Trained Language Models
  - [ICLR 23] Encoding Recurrence into Transformers
  - [ICLR 23] What learning algorithm is in-context learning? Investigations with linear models
  - [ICLR 23] Ask Me Anything: A simple strategy for prompting language models 
  - [ICLR 23] Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language Models 
  - [ICLR 23] PEER: A Collaborative Language Model 
  - [ICLR 23] CodeBPE: Investigating Subtokenization Options for Large Language Model Pretraining on Source Code
  - [ICLR 23] Preserving Pre-trained Features Helps Calibrate Fine-tuned Language Models
  - [ICLR 23] Compositional Task Representations for Large Language Models 
  - [ICLR 23] Language models are multilingual chain-of-thought reasoners 
  - [ICLR 23] Bidirectional Language Models Are Also Few-shot Learners
  - [ICLR 23] Copy is All You Need 
  - [ICLR 23] Parameter-Efficient Fine-Tuning Design Spaces 
  

## Information Extraction
  - [ACL 22] Open Domain Question Answering with A Unified Knowledge Interface
  - [ACL 22] Dynamic Prefix-Tuning for Generative Template-based Event Extraction
  - [ACL 22] Dynamic Global Memory for Document-level Argument Extraction
  - [ACL 22] Pre-training to Match for Unified Low-shot Relation Extraction
  - [ACL 22] Prompt for Extraction? PAIE: Prompting Argument Interaction for Event Argument Extraction
  - [ACL 22] Improving Relation Extraction through Syntax-induced Pre-training with Dependency Masking
  - [EMNLP 22] Large language models are few-shot clinical information extractors
  - [EMNLP 22] Syntactically Rich Discriminative Training: An Effective Method for Open Information Extraction
  - [EMNLP 22] Bi-Directional Iterative Prompt-Tuning for Event Argument Extraction
  - [EMNLP 22] Better Few-Shot Relation Extraction with Label Prompt Dropout
  - [EMNLP 22] Towards Better Document-level Relation Extraction via Iterative Inference
  - [EMNLP 22] IELM: An Open Information Extraction Benchmark for Pre-Trained Language Models
  - [EMNLP 22] CN-AutoMIC: Distilling Chinese Commonsense Knowledge from Pretrained Language Models
  - [EMNLP 22] Thinking about GPT-3 In-Context Learning for Biomedical IE? Think Again
  - [EMNLP 22] Open-Vocabulary Argument Role Prediction For Event Extraction
 

## Bias / Social Media NLP
  - [ACL 22] Toward Annotator Group Bias in Crowdsourcing
  - [ACL 22] An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models
  - [ACL 22] Transformers in the loop: Polarity in neural models of language
  - [ACL 22] Richer Countries and Richer Representations
 

## Generation(Dialog, Summarization, Keyphrase)
  - [ACL 22] Mix and Match: Learning-free Controllable Text Generationusing Energy Language Models 
  - [ACL 22] Faithful or Extractive? On Mitigating the Faithfulness-Abstractiveness Trade-off in Abstractive Summarization
  - [ACL 22] Spurious Correlations in Reference-Free Evaluation of Text Generation
  - [ACL 22] CTRLEval: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation
  - [ACL 22] There Are a Thousand Hamlets in a Thousand People’s Eyes: Enhancing Knowledge-grounded Dialogue with Personal Memory
  - [ACL 22] A Recipe for Arbitrary Text Style Transfer with Large Language Models
  - [EMNLP 22] A Distributional Lens for Multi-Aspect Controllable Text Generation
  - [EMNLP 22] Prompt-and-Rerank: A Method for Zero-Shot and Few-Shot Arbitrary Textual Style Transfer with Small Language Models
  - [EMNLP 22] Natural Language to Code Translation with Execution
  - [EMNLP 22] The Authenticity Gap in Human Evaluation
  - [EMNLP 22] InstructDial: Improving Zero and Few-shot Generalization in Dialogue through Instruction Tuning
  - [EMNLP 22] Hierarchical Phrase-Based Sequence-to-Sequence Learning
  - [EMNLP 22] Overcoming Catastrophic Forgetting in Zero-Shot Cross-Lingual Generation
  - [EMNLP 22] Model Criticism for Long-Form Text Generation
  - [EMNLP 22] Sequentially Controlled Text Generation
  - [EMNLP 22] Binding Language Models in Symbolic Languages
  - [ICLR 23] Calibrating Sequence likelihood Improves Conditional Language Generation 
 

## Structured Prediction
  - [ACL 22] On The Ingredients of an Effective Zero-shot Semantic Parser
  - [ACL 22] Continual Pre-training of Language Models for Math Problem Understanding with Syntax-Aware Memory Network
  - [ACL 22] Substructure Distribution Projection for Zero-Shot Cross-Lingual Dependency Parsing
  - [ACL 22] DeepStruct: Pretraining of Language Models for Structure Prediction
  - [EMNLP 22] Autoregressive Structured Prediction with Language Models
 
