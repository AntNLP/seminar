## Few-shot/Zero-shot Learning, Self-supervised Learning, Contrastive Learning, Life-long Learning, Active Learning 
  - [ACL 21] Refining Sample Embeddings with Relation Prototypes to Enhance Continual Relation Extraction
  - [ACL 21] Unsupervised Out-of-Domain Detection via Pre-trained Transformers
  - [ACL 21] UXLA: A Robust Unsupervised Data Augmentation Framework for Zero-Resource Cross-Lingual NLP
  - [ACL 21] Rational LAMOL: A Rationale-based Lifelong Learning Framework
  - [ACL 21] Meta-KD: A Meta Knowledge Distillation Framework for Language Model Compression across Domains
  - [ACL 21] Few-Shot Question Answering by Pretraining Span Selection
  - [ACL 21] Few-NERD: A Few-shot Named Entity Recognition Dataset
  - [ACL 21] Parameter-Efficient Transfer Learning with Diff Pruning
  - [ACL 21] A Closer Look at Few-Shot Crosslingual Transfer: The Choice of Shots Matters
  - [EMNLP 21] Efficient Contrastive Learning via Novel Data Augmentation and Curriculum Learning
  - [EMNLP 21] Lifelong Explainer for Lifelong Learners
  - [EMNLP 21] Lifelong Event Detection with Knowledge Transfer
  - [EMNLP 21] STraTA: Self-Training with Task Augmentation for Better Few-shot Learning
  - [EMNLP 21] Diverse Distributions of Self-Supervised Tasks for Meta-Learning in NLP
  - [EMNLP 21] Revisiting Self-training for Few-shot Learning of Language Model
  - [EMNLP 21] Self-training with Few-shot Rationalization
  - [ICLR 21] Lifelong Learning of Compositional Structures
  - [ICLR 22] Pretrained Language Model in Continual Learning: A Comparative Study 


## Application
  - [ACL 21] PASS: Perturb-and-Select Summarizer for Product Reviews
  - [ACL 21] Can vectors read minds better than experts? Comparing data augmentation strategies for the automated scoring of children’s mindreading ability
  - [ACL 21] Measuring Conversational Uptake: A Case Study on Student-Teacher Interactions
  - [ACL 21] A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies
  - [ACL 21] Explaining Relationships Between Scientific Documents
  - [ACL 21] PLOME: Pre-training with Misspelled Knowledge for Chinese Spelling Correction
  - [ACL 21] Best of Both Worlds: Making High Accuracy Non-incremental Transformer-based Disfluency Detection Incremental
  - [ACL 21] Learning Language and Multimodal Privacy-Preserving Markers of Mood from Mobile Data
  - [ACL 21] Tail-to-Tail Non-Autoregressive Sequence Prediction for Chinese Grammatical Error Correction
  - [ACL 21] What is Your Article Based On? Inferring Fine-grained Provenance
  - [ACL 21] Instantaneous Grammatical Error Correction with Shallow Aggressive Decoding
  - [ACL 21] Exploration and Exploitation: Two Ways to Improve Chinese Spelling Correction Models
  - [EMNLP 21] Competency Problems: On Finding and Removing Artifacts in Language Data
  - [EMNLP 21] Efficient-FedRec: Efficient Federated Learning Framework for Privacy-Preserving News Recommendation
  - [EMNLP 21] Is this the end of the gold standard? A straightforward reference-less grammatical error correction metric
  - [EMNLP 21] Self-Supervised Curriculum Learning for Spelling Error Correction
  - [EMNLP 21] SpellBERT: A Lightweight Pretrained Model for Chinese Spelling Check
  - [EMNLP 21] LM-Critic: Language Models for Unsupervised Grammatical Error Correction
  - [EMNLP 21] Multi-Class Grammatical Error Detection for Correction: A Tale of Two Systems



## Language Grounding, Multi-modal
  - [ACL 21] E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning
  - [ACL 21] Learning Relation Alignment for Calibrated Cross-modal Retrieval
  - [ACL 21] UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning
  - [EMNLP 21] Effect of Visual Extensions on Natural Language Understanding in Vision-and-Language Models
  - [EMNLP 21] Improving Pre-trained Vision-and-Language Embeddings for Phrase Grounding 
  - [EMNLP 21] Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers



## Interpretability, Robustness, Compressing, Privacy
  - [ACL 21] Learning Language Specific Sub-network for Multilingual Machine Translation
  - [ACL 21] Introducing Orthogonal Constraint in Structural Probes
  - [ACL 21] Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger
  - [ACL 21] Examining the Inductive Bias of Neural Language Models with Artificial Languages
  - [ACL 21] Explaining Contextualization in Language Models using Visual Analytics
  - [ACL 21] Cascaded Head-colliding Attention
  - [ACL 21] Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks
  - [ACL 21] Do Context-Aware Translation Models Pay the Right Attention?
  - [ACL 21] What Context Features Can Transformer Language Models Use?
  - [ACL 21] Integrated Directional Gradients: Feature Interaction Attribution for Neural NLP Models
  - [ACL 21] Analyzing the Source and Target Contributions to Predictions in Neural Machine Translation
  - [ACL 21] Comparing Test Sets with Item Response Theory
  - [ACL 21] Uncovering Constraint-Based Behavior in Neural Models via Targeted Fine-Tuning
  - [ACL 21] Causal Analysis of Syntactic Agreement Mechanisms in Neural Language Models
  - [ACL 21] Bird’s Eye: Probing for Linguistic Graph Structures with a Simple Information-Theoretic Approach
  - [ACL 21] Contributions of Transformer Attention Heads in Multi- and Cross-lingual Tasks
  - [ACL 21] Weight Distillation: Transferring the Knowledge in Neural Network Parameters
  - [ACL 21] Changing the World by Changing the Data
  - [ACL 21] Structural Guidance for Transformer Language Models
  - [ACL 21] Anonymisation Models for Text Data: State of the art, Challenges and Future Directions
  - [ACL 21] How is BERT surprised? Layerwise detection of linguistic anomalies
  - [ACL 21] BinaryBERT: Pushing the Limit of BERT Quantization
  - [ACL 21] AutoTinyBERT: Automatic Hyper-parameter Optimization for Efficient Pre-trained Language Models
  - [ACL 21] A Cognitive Regularizer for Language Modeling
  - [ACL 21] Lower Perplexity is Not Always Human-Like
  - [ACL 21] Language Model Evaluation Beyond Perplexity
  - [ACL 21] Super Tickets in Pre-Trained Language Models: From Model Compression to Improving Generalization
  - [ACL 21] StructFormer: Joint Unsupervised Induction of Dependency and Constituency Structure from Masked Language Modeling
  - [ACL 21] Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning
  - [ACL 21] Vocabulary Learning via Optimal Transport for Neural Machine Translation
  - [ACL 21] Is Sparse Attention more Interpretable?
  - [ACL 21] How effective is BERT without word ordering? Implications for language understanding and data privacy
  - [ACL 21] Robust Transfer Learning with Pretrained Language Models through Adapters
  - [EMNLP 21] Distilling Linguistic Context for Language Model Compression
  - [EMNLP 21] Dynamic Knowledge Distillation for Pre-trained Language Models
  - [EMNLP 21] Do Long-Range Language Models Actually Use Long-Range Context?
  - [EMNLP 21] Frequency Effects on Syntactic Rule Learning in Transformers
  - [EMNLP 21] Contrastive Out-of-Distribution Detection for Pretrained Transformers
  - [EMNLP 21] When differential privacy meets NLP: The devil is in the detail
  - [EMNLP 21] Evaluating the Robustness of Neural Language Models to Input Perturbations
  - [EMNLP 21] How much pretraining data do language models need to learn syntax?
  - [EMNLP 21] Sorting through the noise: Testing robustness of information processing in pre-trained language models
  - [EMNLP 21] Contrastive Explanations for Model Interpretability
  - [EMNLP 21] Conditional probing: measuring usable information beyond a baseline
  - [EMNLP 21] Layer-wise Model Pruning based on Mutual Information
  - [EMNLP 21] Chinese WPLC: A Chinese Dataset for Evaluating Pretrained Language Models on Word Prediction Given Long-Range Context
  - [EMNLP 21] Incorporating Residual and Normalization Layers into Analysis of Masked Language Models
  - [EMNLP 21] Examining Cross-lingual Contextual Embeddings with Orthogonal Structural Probes
  - [EMNLP 21] Profanity-Avoiding Training Framework for Seq2seq Models with Certified Robustness
  - [EMNLP 21] Transformer Feed-Forward Layers Are Key-Value Memories
  - [EMNLP 21] How Do Neural Sequence Models Generalize? Local and Global Cues for Out-of-Distribution Prediction
  - [EMNLP 21] Controlled Evaluation of Grammatical Knowledge in Mandarin Chinese Language Models
  - [EMNLP 21] Inducing Transformer’s Compositional Generalization Ability via Auxiliary Sequence Prediction Tasks
  - [EMNLP 21] Editing Factual Knowledge in Language Models
  - [EMNLP 21] Generating Datasets with Pretrained Language Models
  - [EMNLP 21] On the Influence of Masking Policies in Intermediate Pre-training
  - [EMNLP 21] Understanding and Overcoming the Challenges of Efficient Transformer Quantization
  - [EMNLP 21] Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience
  - [EMNLP 21] What’s in Your Head? Emergent Behaviour in Multi-Task Transformer Models
  - [EMNLP 21] Block Pruning For Faster Transformers
  - [EMNLP 21] Finetuning Pretrained Transformers into RNNs
  - [EMNLP 21] Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of BERT Compression
  - [ICLR 21] Neural Pruning via Growing Regularization 
  - [ICLR 21] On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines 
  - [ICLR 21] Predicting Inductive Biases of Pre-Trained Models
  - [ICLR 21] InfoBERT: Improving Robustness of Language Models from An Information Theoretic Perspective
  - [ICLR 22] Fine-Tuning Distorts Pretrained Features and Underperforms Out-of-Distribution
  - [ICLR 22] Revisiting Over-smoothing in BERT from the Perspective of Graph 
  - [ICLR 22] 8-bit Optimizers via Block-wise Quantization 
  - [ICLR 22] The MultiBERTs: BERT Reproductions for Robustness Analysis 
  - [ICLR 22] Multitask Prompted Training Enables Zero-Shot Task Generalization 
  - [ICLR 22] BiBERT: Accurate Fully Binarized BERT 
  - [ICLR 22] Exploring extreme parameter compression for pre-trained language models 
  - [ICLR 22] Fine-Tuning Distorts Pretrained Features and Underperforms Out-of-Distribution
  - [ICLR 22] Language model compression with weighted low-rank factorization
  - [ICLR 22] Discovering Latent Concepts Learned in BERT 
  - [ICLR 22] On the Pitfalls of Analyzing Individual Neurons in Language Models
  - [NeurIPS 21] Mind the Gap: Assessing Temporal Generalization in Neural Language Models
  - [NeurIPS 21] Influence Patterns for Explaining Information Flow in BERT 



## Machine Translation, Multi-(Cross-)lingual Tasks
  - [ACL 21] Bilingual Lexicon Induction via Unsupervised Bitext Construction and Word Alignment
  - [ACL 21] Improving Zero-Shot Translation by Disentangling Positional Information
  - [ACL 21] Diverse Pretrained Context Encodings Improve Document Translation
  - [ACL 21] Glancing Transformer for Non-Autoregressive Neural Machine Translation
  - [ACL 21] How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models
  - [ACL 21] Evaluating morphological typology in zero-shot cross-lingual transfer
  - [ACL 21] CCMatrix: Mining Billions of High-Quality Parallel Sentences on the Web
  - [EMNLP 21] Improving Zero-Shot Cross-Lingual Transfer Learning via Robust Training
  - [EMNLP 21] Cross-Attention is All You Need: Adapting Pretrained Transformers for Machine Translation
  - [EMNLP 21] Learning to Rewrite for Non-Autoregressive Neural Machine Translation
  - [EMNLP 21] Genre as Weak Supervision for Cross-lingual Dependency Parsing
  - [EMNLP 21] Machine Translation Decoding beyond Beam Search
  - [ICLR 21] Nearest Neighbor Machine Translation


## Pre-trained Models, Embeddings
  - [ACL 21] Selecting Informative Contexts Improves Language Model Fine-tuning
  - [ACL 21] When Do You Need Billions of Words of Pretraining Data?
  - [ACL 21] Reflective Decoding: Beyond Unidirectional Generation with Off-the-Shelf Language Models
  - [ACL 21] Optimizing Deeper Transformers on Small Datasets
  - [ACL 21] On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation
  - [ACL 21] An Empirical Study on Hyperparameter Optimization for Fine-Tuning Pre-trained Language Model
  - [ACL 21] Self-Guided Contrastive Learning for BERT Sentence Representations
  - [ACL 21] LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding
  - [ACL 21] LeeBERT: Learned Early Exit for BERT with cross-level optimization
  - [ACL 21] Taming Pre-trained Language Models with N-gram Representations for Low-Resource Domain Adaptation
  - [ACL 21] Consistency Regularization for Cross-Lingual Fine-Tuning
  - [ACL 21] Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word Alignment
  - [ACL 21] H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences
  - [ACL 21] Making Pre-trained Language Models Better Few-shot Learners
  - [ACL 21] TAN-NTM: Topic Attention Networks for Neural Topic Modeling
  - [ACL 21] VECO: Variable and Flexible Cross-lingual Pre-training for Language Understanding and Generation
  - [ACL 21] Reservoir Transformers
  - [ACL 21] Syntax-augmented Multilingual BERT for Cross-lingual Transfer
  - [ACL 21] How to Adapt Your Pretrained Multilingual Model to 1600 Languages
  - [ACL21] Prefix-Tuning: Optimizing Continuous Prompts for Generation
  - [ACL 21] Pre-training Universal Language Representation
  - [ACL 21] Structural Pre-training for Dialogue Comprehension
  - [ACL 21] Enabling Lightweight Fine-tuning for Pre-trained Language Model Compression based on Matrix Product Operators
  - [ACL 21] On Sample Based Explanation Methods for NLP: Faithfulness, Efficiency and Semantic Evaluation
  - [ACL 21] Syntax-Enhanced Pre-trained Model
  - [ACL 21] Shortformer: Better Language Modeling using Shorter Inputs
  - [ACL 21] GhostBERT: Generate More Features with Cheap Operations for BERT
  - [ACL 21] Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search
  - [ACL 21] Don’t Let Discourse Confine Your Model: Sequence Perturbations for Improved Event Language Models
  - [EMNLP 21] Zero-Shot Cross-Lingual Transfer of Neural Machine Translation with Multilingual Pretrained Encoders
  - [EMNLP 21] ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora
  - [EMNLP 21] Improving Sequence-to-Sequence Pre-training via Sequence Span Rewriting
  - [EMNLP 21] The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers
  - [EMNLP 21] Disentangling Representations of Text by Masking Transformers
  - [EMNLP 21] Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders
  - [EMNLP 21] RuleBERT: Teaching Soft Rules to Pre-Trained Language Models
  - [EMNLP 21] mT6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs
  - [EMNLP 21] Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent
  - [EMNLP 21] Sentence Bottleneck Autoencoders from Transformer Language Models
  - [EMNLP 21] Fast WordPiece Tokenization
  - [EMNLP 21] What’s Hidden in a One-layer Randomly Weighted Transformer?
  - [EMNLP 21] A Simple and Effective Positional Encoding for Transformers
  - [EMNLP 21] GAML-BERT: Improving BERT Early Exiting by Gradient Aligned Mutual Learning
  - [EMNLP 21] The Power of Scale for Parameter-Efficient Prompt Tuning
  - [EMNLP 21] Frustratingly Simple Pretraining Alternatives to Masked Language Modeling
  - [EMNLP 21] Allocating Large Vocabulary Capacity for Cross-Lingual Language Model Pre-Training
  - [EMNLP 21] SHAPE: Shifted Absolute Position Embedding for Transformers
  - [EMNLP 21] CTAL: Pre-training Cross-modal Transformer for Audio-and-Language Representations
  - [EMNLP 21] Mixture-of-Partitions: Infusing Large Biomedical Knowledge Graphs into BERT
  - [EMNLP 21] AVocaDo: Strategy for Adapting Vocabulary to Downstream Domain
  - [EMNLP 21] Sparse Attention with Linear Units
  - [EMNLP 21] When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute
  - [EMNLP 21] Sequence Length is a Domain: Length-based Overfitting in Transformer Models
  - [EMNLP 21] Effective Fine-Tuning Methods for Cross-lingual Adaptation
  - [EMNLP 21] Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning
  - [EMNLP 21] Value-aware Approximate Attention
  - [ICLR 21] Taking Notes on the Fly Helps Language Pre-Training
  - [ICLR 21] MixKD: Towards Efficient Distillation of Large-scale Language Models
  - [ICLR 21] Variational Information Bottleneck for Effective Low-Resource Fine-Tuning
  - [ICLR 21] DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION
  - [ICLR 21] Revisiting Few-sample BERT Fine-tuning
  - [ICLR 21] Rethinking Embedding Coupling in Pre-trained Language Models
  - [ICLR 21] DeLighT: Deep and Light-weight Transformer
  - [ICLR 21] BERTology Meets Biology: Interpreting Attention in Protein Language Models
  - [ICLR 21] Better Fine-Tuning by Reducing Representational Collapse
  - [ICLR 21] Anchor & Transform: Learning Sparse Embeddings for Large Vocabularies 
  - [ICLR 21] On Position Embeddings in BERT
  - [ICLR 21] Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning
  - [ICLR 22] Language modeling via stochastic processes
  - [ICLR 22] Finetuned Language Models are Zero-Shot Learners
  - [ICLR 22] BEiT: BERT Pre-Training of Image Transformers 
  - [ICLR 22] Compositional Attention: Disentangling Search and Retrieval 
  - [ICLR 22] GreaseLM: Graph REASoning Enhanced Language Models
  - [ICLR 22] Memorizing Transformers
  - [ICLR 22] Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation 
  - [ICLR 22] cosFormer: Rethinking Softmax In Attention 
  - [ICLR 22] Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners 
  - [ICLR 22] On Robust Prefix-Tuning for Text Classification 
  - [ICLR 22] DictFormer: Tiny Transformer with Shared Dictionary 
  - [ICLR 22] Should We Be Pre-training? An Argument for End-task Aware Training as an Alternative 
  - [ICLR 22] HTLM: Hyper-Text Pre-Training and Prompting of Language Models 
  - [ICLR 22] LoRA: Low-Rank Adaptation of Large Language Models
  - [ICLR 22] Rethinking Supervised Pre-Training for Better Downstream Transferring
  - [ICLR 22] No Parameters Left Behind: Sensitivity Guided Adaptive Learning Rate for Training Large Transformer Models 
  - [ICLR 22] Fast Model Editing at Scale
  - [ICLR 22] Mention Memory: incorporating textual knowledge into Transformers through entity mention attention 
  - [NeurIPS 21] Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning
  - [NeurIPS 21] Sparse is Enough in Scaling Transformers
  - [NeurIPS 21] Searching for Efficient Transformers for Language Modeling
  - [NeurIPS 21] Luna: Linear Unified Nested Attention
  - [NeurIPS 21] FMMformer: Efficient and Flexible Transformer via Decomposed Near-field and Far-field Attention 
  - [NeurIPS 21] COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining


## Information Extraction
  - [ACL 21] DESCGEN: A Distantly Supervised Datasetfor Generating Entity Descriptions
  - [ACL 21] From Discourse to Narrative: Knowledge Projection for Event Relation Extraction
  - [ACL 21] Ultra-Fine Entity Typing with Weak Supervision from a Masked Language Model
  - [ACL 21] Element Intervention for Open Relation Extraction
  - [ACL 21] CoRI: Collective Relation Integration with Data Augmentation for Open Information Extraction
  - [ACL 21] Exploring Distantly-Labeled Rationales in Neural Network Models
  - [ACL 21] Joint Biomedical Entity and Relation Extraction with Knowledge-Enhanced Collective Inference
  - [EMNLP 21] Everything Is All It Takes: A Multipronged Strategy for Zero-Shot Cross-Lingual Information Extraction
  - [EMNLP 21] CodRED: A Cross-Document Relation Extraction Dataset for Acquiring Knowledge in the Wild
  - [EMNLP 21] Towards Realistic Few-Shot Relation Extraction
  - [EMNLP 21] Distilling Relation Embeddings from Pretrained Language Models


## Bias / Social Media NLP
  - [ACL 21] Understanding and Countering Stereotypes: A Computational Approach to the Stereotype Content Model
  - [ACL 21] Breaking Down the Invisible Wall of Informal Fallacies in Online Discussions
  - [ACL 21] Stereotyping Norwegian Salmon: An Inventory of Pitfalls in Fairness Benchmark Datasets
  - [ACL 21] StereoSet: Measuring stereotypical bias in pretrained language models
  - [EMNLP 21] Debiasing Methods in Natural Language Understanding Make Bias More Accessible
  - [EMNLP 21] Sociolectal Analysis of Pretrained Language Models

## Generation(Dialog, Summarization, Keyphrase)
  - [ACL 21] Mention Flags (MF): Constraining Transformer-based Text Generators
  - [ACL 21] Learning from Perturbations: Diverse and Informative Dialogue Generation with Inverse Adversarial Training
  - [ACL 21] Increasing Faithfulness in Knowledge-Grounded Dialogue with Controllable Features
  - [ACL 21] Style is NOT a single variable: Case Studies for Cross-Stylistic Language Understanding
  - [ACL 21] TextSETTR: Few-Shot Text Style Extraction and Tunable Targeted Restyling
  - [ACL 21] Transfer Learning for Sequence Generation: from Single-source to Multi-source
  - [ACL 21] Guiding the Growth: Difficulty-Controllable Question Generation through Step-by-Step Rewriting
  - [ACL 21] Determinantal Beam Search
  - [ACL 21] DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts
  - [ACL 21] Thank you BART! Rewarding Pre-Trained Models Improves Formality Style Transfer
  - [EMNLP 21] Few-Shot Text Generation with Natural Language Instructions
  - [EMNLP 21] Conditional Poisson Stochastic Beams
  - [EMNLP 21] Transductive Learning for Unsupervised Text Style Transfer
  - [EMNLP 21] Paraphrase Generation: A Survey of the State of the Art
  - [EMNLP 21] Unsupervised Paraphrasing with Pretrained Language Models
  - [EMNLP 21] Compression, Transduction, and Creation: A Unified Framework for Evaluating Natural Language Generation
  - [EMNLP 21] Exploring Non-Autoregressive Text Style Transfer
  - [ICLR 21] Text Generation by Learning from Demonstrations
  - [ICLR 21] A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks
  - [ICLR 21] CoCon: A Self-Supervised Approach for Controlled Text Generation
  - [ICLR 21] What they do when in doubt: a study of inductive biases in seq2seq learners
  - [ICLR 21] Understanding and Improving Encoder Layer Fusion in Sequence-to-Sequence Learning 
  - [ICLR 22] Non-Parallel Text Style Transfer with Self-Parallel Supervision
  - [NeurIPS 21] Duplex Sequence-to-Sequence Learning for Reversible Machine Translation


## Structured Prediction
  - [ACL 21] Span-based Semantic Parsing for Compositional Generalization
  - [ACL 21] Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?
  - [ACL 21] Meta-Learning to Compositionally Generalize
  - [ACL 21] Value-Agnostic Conversational Semantic Parsing
  - [ACL 21] On Compositional Generalization of Neural Machine Translation
  - [EMNLP 21] Constrained Language Models Yield Few-Shot Semantic Parsers
  - [EMNLP 21] Efficient Sampling of Dependency Structure
  - [EMNLP 21] Finding needles in a haystack: Sampling Structurally-diverse Training Sets from Synthetic Data for Compositional Generalization 
  - [ICLR 21] Learning to Recombine and Resample Data For Compositional Generalization 
  - [ICLR 21] Uncertainty Estimation in Autoregressive Structured Prediction 
  - [NeurIPS 21] Sequence-to-Sequence Learning with Latent Neural Grammars

